# 5.13

### Black magic
Для пункта 1 воспользовался встроенной функцией
~~~
curve_fit(func, x[], y[])
~~~
Т.к. тот же самый метод реализован в более сложном варианте в пункте 3. 
### Градиентный спуск
Для пункта 3 написал простейший градиентный спуск:
вычисляем градиент, по формуле центральной разности. Затем идем против градиента, в поисках экстремума. Сразу же оказываемся в минимуме, который и соответсвует искомому набору коэффициентов. Результат:  ![GitHub Logo](https://github.com/Flammifer/Cmath/blob/master/5.13/Figure_1.png)
Зеленый - МНК и линейная функция. Красный - МНК и четыре непрерывных функции из условия задачи.

Стоит обратить внимание, что когда спускаемся по градиенту, необходимо его брать с небольшим весом, т.к. иначе "пролетаем" экстремум. 
Можно сделать вывод, что градиентный спуск легко масштабируем (в питоне это легко сделать, отправляя список функций), и работает очень быстро (сошлось за 44 итерации) 
Итоговые ~~веса~~ коэффициенты:
~~~~
[414.97677963964793, 520.4849637382838, 880.4834985984286, 1111.1107422884866]
~~~~

### Покомпонентный спуск

Работает медленнее обычного ГС.

Кроме того, как выяснилось покомпонентный спуск менее чувствителен к выбору lambda_. Что, несомненно, большое преимушество. 
При lambda_= 0.1 обычный градиентный спуск расходится, а покомпонетный нет

#### Дробление коэффициента
Т.к. Возникают сомнения по поводу универсальности выбора коэффициента lambda_, который отвечает за продвижение по антиградиенту, принял решение о использовании дробления коэффицинта, согласно сайтам о machine learning этот способ позволяет надежнее решать многие задачи поиска экстремума.

### Наискорейший спуск

В файле 5-13.py реализован наискорейший спуск. 
Сходится очень быстро для любой точности, сам решает задачу выбора lambda_. 
Этот алгоритм мой бро. 

|                | Градиентный спуск | Покомпонетный спуск | Наискорейший |
|----------------|-------------------|---------------------|--------------|
|epsilon = 0.0001| 44                | 449                 |       22     |



